# Povezivanje na Lokalni AI Server (LM Studio)

U prethodnim lekcijama, naša aplikacija je komunicirala sa cloud-based AI servisima poput Gemini-ja. U ovoj lekciji, naučit ćemo kako našu aplikaciju prebaciti na potpuno lokalni sistem, gdje se AI modeli 
izvršavaju direktno na našem računaru. Ovo nam daje potpunu privatnost, neograničeno i besplatno korištenje, te mogućnost rada bez internet konekcije.

## 1. Uvod u LM Studio i lokalno pokretanje modela

![image.png](/files/images/07_image.png)

**1. Šta je LM Studio?**

- **LM Studio** je besplatna aplikacija za Windows, Mac i Linux koja vam omogućava da
jednostavno preuzimate i pokrećete open-source jezičke modele (LLM) na
svom računaru. [https://lmstudio.ai/](https://lmstudio.ai/)
- **Ključne prednosti:**
    - **Privatnost:** Svi podaci ostaju na vašem računaru.
    - **Offline rad:** Jednom kada preuzmete model, internet vam nije potreban.
    - **Nema troškova:** Korištenje modela je potpuno besplatno.
    - **Lokalni server:** LM Studio može pokrenuti server koji se ponaša kao OpenAI API, što nam
    omogućava da našu postojeću aplikaciju vrlo lako povežemo s njim.

**2. Preuzimanje i pokretanje prvog modela:**

- Preuzmite i instalirajte [**LM Studio**](https://www.google.com/url?sa=E&q=https%3A%2F%2Flmstudio.ai%2F) sa njihove zvanične stranice.
- Pokrenite aplikaciju i idite na početnu stranicu.
- U polje za pretragu (Discover), ukucajte ime nekog manjeg, efikasnog modela, npr. **Qwen 1.7B Chat**.
- Iz liste rezultata, odaberite jednu od verzija i kliknite **Download**.
- Nakon preuzimanja, idite na tab za chat (ikona oblačića 💬). U vrhu, odaberite model koji ste upravo preuzeli i testirajte ga direktno u aplikaciji.

## 2. Pokretanje lokalnog servera i povezivanje aplikacije

![image.png](/files/images/07_image%201.png)

Ovo je ključni dio. Sada ćemo "otvoriti vrata" našem lokalnom modelu kako bi naša Streamlit aplikacija mogla razgovarati s njim.

**1. Pokretanje servera u LM Studio:**

- Idite na tab za lokalni server (ikona Terminala).
- Na vrhu, iz padajućeg menija odaberite model koji želite koristiti (npr. Qwen 1.8B Chat).
- Kliknite na settings i podesite port 12345. Zatim kliknite na dugme **Start Server**.
- LM Studio će sada "slušati" zahtjeve na lokalnoj adresi, najčešće http://127.0.0.1:12345. Zabilježite ovu adresu i port.

**2. Modifikacija** app.py **koda za povezivanje:**

- Otvorite vaš projekat i fajl app.py u Visual Studio Code-u.
- Moramo napraviti dvije ključne izmjene u dijelu koda koji definiše OpenAI klijenta.
    
    **Izmjena 1: Promjena adrese servera (base_url)**
    
    - Pronađite liniju gdje se definiše base_url.
    - **Originalni kod (za OpenRouter) je izgledao ovako:** code Python
        
        ```python
        base_url="https://openrouter.ai/api/v1"
        ```    *   **Zamijenite je sa adresom vašeg lokalnog LM Studio servera:**
        ```python
        base_url="http://127.0.0.1:12345/v1"
        ```
        
- **VAŽNO:** Obratite pažnju da sada koristimo http umjesto https, jer lokalni server nije zaštićen SSL certifikatom.

**Izmjena 2: API ključ**

- Lokalni server ne zahtijeva API ključ. Međutim, OpenAI biblioteka očekuje da
ključ postoji. Zato ćemo mu dati bilo kakav tekst, npr. "not-needed".
    
    ```
    api_key="not-needed"
    ```
    

## 3. Prilagođavanje koda za lokalni Server

**Problem:**
Nakon izmjena, aplikacija radi, ali lista modela je prazna! Zašto? Zato što naš lokalni server vraća listu modela u drugačijem formatu nego OpenRouter. Moramo prilagoditi funkciju koja ih učitava.

Svaki put moramo provjeriti šta nam vraća određi API, tj. kakav JSON sadržaj pa shodno tome moramo prilagoditi naš kod.

**Rješenje:** 

Modifikacija funkcije get_models()

- Pronađite funkciju get_models() u app.py.
- **Originalni kod je filtrirao modele koji su besplatni:** code Python
    
    ```python
    # Stari kod
    free_models = [model['id'] for model in models_data['data'] if model.get('pricing', {}).get('prompt') == '0']
    return free_models
    ```
    
- **LM Studio API je jednostavniji.** On vraća listu modela gdje svaki model ima polje id. Moramo samo izvući te ID-jeve.
- **Zamijenite stari kod sa novim, jednostavnijim kodom:** code
    
    ```python
    # Novi kod
    model_ids = [model['id'] for model in models_data['data']]
    return model_ids
    ```
    
- *Savjet: Ovu izmjenu možete uraditi i uz pomoć AI asistenta, tako što ćete mu reći: "Modify the get_models function to get only the 'id' field from the list, without checking for pricing."*

## 4. Testiranje i Zaključak

Sada je sve spremno za finalni test.

1. Provjerite da li je server u **LM Studio** još uvijek pokrenut.
2. Sačuvajte sve izmjene u **app.py**.
3. Ponovo pokrenite vašu Streamlit aplikaciju: streamlit run app.py.
4. **Uspjeh!** Padajući meni za odabir modela sada prikazuje modele koji su dostupni u vašem LM Studio.
    
    ![image.png](/files/images/10_fc6aa660-e4f1-4ab0-ad8b-eb4acb064de5.png)
    
5. Odaberite model (npr. Qwen) i pošaljite mu upit.
6. Pratite logove u LM Studio – vidjet ćete kako zahtjev stiže, model ga obrađuje i šalje odgovor nazad. Odgovor će se pojaviti u vašoj chatbot aplikaciji.