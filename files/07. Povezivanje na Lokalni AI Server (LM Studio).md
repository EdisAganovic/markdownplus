# Povezivanje na Lokalni AI Server (LM Studio)

U prethodnim lekcijama, naÅ¡a aplikacija je komunicirala sa cloud-based AI servisima poput Gemini-ja. U ovoj lekciji, nauÄit Ä‡emo kako naÅ¡u aplikaciju prebaciti na potpuno lokalni sistem, gdje se AI modeli 
izvrÅ¡avaju direktno na naÅ¡em raÄunaru. Ovo nam daje potpunu privatnost, neograniÄeno i besplatno koriÅ¡tenje, te moguÄ‡nost rada bez internet konekcije.

## 1. Uvod u LM Studio i lokalno pokretanje modela

![image.png](/files/images/07_image.png)

**1. Å ta je LM Studio?**

- **LM Studio** je besplatna aplikacija za Windows, Mac i Linux koja vam omoguÄ‡ava da
jednostavno preuzimate i pokreÄ‡ete open-source jeziÄke modele (LLM) na
svom raÄunaru. [https://lmstudio.ai/](https://lmstudio.ai/)
- **KljuÄne prednosti:**
    - **Privatnost:** Svi podaci ostaju na vaÅ¡em raÄunaru.
    - **Offline rad:** Jednom kada preuzmete model, internet vam nije potreban.
    - **Nema troÅ¡kova:** KoriÅ¡tenje modela je potpuno besplatno.
    - **Lokalni server:** LM Studio moÅ¾e pokrenuti server koji se ponaÅ¡a kao OpenAI API, Å¡to nam
    omoguÄ‡ava da naÅ¡u postojeÄ‡u aplikaciju vrlo lako poveÅ¾emo s njim.

**2. Preuzimanje i pokretanje prvog modela:**

- Preuzmite i instalirajte [**LM Studio**](https://www.google.com/url?sa=E&q=https%3A%2F%2Flmstudio.ai%2F) sa njihove zvaniÄne stranice.
- Pokrenite aplikaciju i idite na poÄetnu stranicu.
- U polje za pretragu (Discover), ukucajte ime nekog manjeg, efikasnog modela, npr. **Qwen 1.7B Chat**.
- Iz liste rezultata, odaberite jednu od verzija i kliknite **Download**.
- Nakon preuzimanja, idite na tab za chat (ikona oblaÄiÄ‡a ğŸ’¬). U vrhu, odaberite model koji ste upravo preuzeli i testirajte ga direktno u aplikaciji.

## 2. Pokretanje lokalnog servera i povezivanje aplikacije

![image.png](/files/images/07_image%201.png)

Ovo je kljuÄni dio. Sada Ä‡emo "otvoriti vrata" naÅ¡em lokalnom modelu kako bi naÅ¡a Streamlit aplikacija mogla razgovarati s njim.

**1. Pokretanje servera u LM Studio:**

- Idite na tab za lokalni server (ikona Terminala).
- Na vrhu, iz padajuÄ‡eg menija odaberite model koji Å¾elite koristiti (npr. Qwen 1.8B Chat).
- Kliknite na settings i podesite port 12345. Zatim kliknite na dugme **Start Server**.
- LM Studio Ä‡e sada "sluÅ¡ati" zahtjeve na lokalnoj adresi, najÄeÅ¡Ä‡e http://127.0.0.1:12345. ZabiljeÅ¾ite ovu adresu i port.

**2. Modifikacija** app.py **koda za povezivanje:**

- Otvorite vaÅ¡ projekat i fajl app.py u Visual Studio Code-u.
- Moramo napraviti dvije kljuÄne izmjene u dijelu koda koji definiÅ¡e OpenAI klijenta.
    
    **Izmjena 1: Promjena adrese servera (base_url)**
    
    - PronaÄ‘ite liniju gdje se definiÅ¡e base_url.
    - **Originalni kod (za OpenRouter) je izgledao ovako:** code Python
        
        ```python
        base_url="https://openrouter.ai/api/v1"
        ```    *   **Zamijenite je sa adresom vaÅ¡eg lokalnog LM Studio servera:**
        ```python
        base_url="http://127.0.0.1:12345/v1"
        ```
        
- **VAÅ½NO:** Obratite paÅ¾nju da sada koristimo http umjesto https, jer lokalni server nije zaÅ¡tiÄ‡en SSL certifikatom.

**Izmjena 2: API kljuÄ**

- Lokalni server ne zahtijeva API kljuÄ. MeÄ‘utim, OpenAI biblioteka oÄekuje da
kljuÄ postoji. Zato Ä‡emo mu dati bilo kakav tekst, npr. "not-needed".
    
    ```
    api_key="not-needed"
    ```
    

## 3. PrilagoÄ‘avanje koda za lokalni Server

**Problem:**
Nakon izmjena, aplikacija radi, ali lista modela je prazna! ZaÅ¡to? Zato Å¡to naÅ¡ lokalni server vraÄ‡a listu modela u drugaÄijem formatu nego OpenRouter. Moramo prilagoditi funkciju koja ih uÄitava.

Svaki put moramo provjeriti Å¡ta nam vraÄ‡a odreÄ‘i API, tj. kakav JSON sadrÅ¾aj pa shodno tome moramo prilagoditi naÅ¡ kod.

**RjeÅ¡enje:** 

Modifikacija funkcije get_models()

- PronaÄ‘ite funkciju get_models() u app.py.
- **Originalni kod je filtrirao modele koji su besplatni:** code Python
    
    ```python
    # Stari kod
    free_models = [model['id'] for model in models_data['data'] if model.get('pricing', {}).get('prompt') == '0']
    return free_models
    ```
    
- **LM Studio API je jednostavniji.** On vraÄ‡a listu modela gdje svaki model ima polje id. Moramo samo izvuÄ‡i te ID-jeve.
- **Zamijenite stari kod sa novim, jednostavnijim kodom:** code
    
    ```python
    # Novi kod
    model_ids = [model['id'] for model in models_data['data']]
    return model_ids
    ```
    
- *Savjet: Ovu izmjenu moÅ¾ete uraditi i uz pomoÄ‡ AI asistenta, tako Å¡to Ä‡ete mu reÄ‡i: "Modify the get_models function to get only the 'id' field from the list, without checking for pricing."*

## 4. Testiranje i ZakljuÄak

Sada je sve spremno za finalni test.

1. Provjerite da li je server u **LM Studio** joÅ¡ uvijek pokrenut.
2. SaÄuvajte sve izmjene u **app.py**.
3. Ponovo pokrenite vaÅ¡u Streamlit aplikaciju: streamlit run app.py.
4. **Uspjeh!** PadajuÄ‡i meni za odabir modela sada prikazuje modele koji su dostupni u vaÅ¡em LM Studio.
    
    ![image.png](/files/images/10_fc6aa660-e4f1-4ab0-ad8b-eb4acb064de5.png)
    
5. Odaberite model (npr. Qwen) i poÅ¡aljite mu upit.
6. Pratite logove u LM Studio â€“ vidjet Ä‡ete kako zahtjev stiÅ¾e, model ga obraÄ‘uje i Å¡alje odgovor nazad. Odgovor Ä‡e se pojaviti u vaÅ¡oj chatbot aplikaciji.